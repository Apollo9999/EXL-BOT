{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is to make or prepare data for NER model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GST Data:\n",
    "# Making the required format for ner:\n",
    "# path to gst data:\n",
    "\n",
    "# As we have the utterances, we take the utterances and tag relevantly:\n",
    "\n",
    "root_data_path = \"D:\\Projects\\MotherBotCategorization\\Data\\FinalData\\GST\"\n",
    "\n",
    "import os\n",
    "# extract only 2 files for now:\n",
    "data = []\n",
    "filenames = os.listdir(root_data_path)\n",
    "for filename in filenames:\n",
    "    with open(root_data_path + \"/\" + filename  ) as file:\n",
    "        content = file.readlines()\n",
    "        content = [line.replace('\\n', '') for line in content]\n",
    "        data.extend(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity keywords \n",
    "entity_values = ['GST', 'GSTIN', 'goods and services tax', 'GST number', 'GST no',\n",
    "'principal place of business', 'main place of business', 'business address','address',\n",
    "'ABBOTT INDIA LIMITED', 'ail', 'ABBOTT HEALTHCARE PRIVATE LIMITED', 'ahpl', 'AHPL', 'AIL',\n",
    "'Gujarat', 'Maharashtra', 'Chattisgarh', \n",
    "             'Uttar Pradesh', 'Delhi', 'Kerala','Tamil Nadu',\n",
    "             'Karnataka','Telangana',\n",
    "            'Andhra Pradesh','GOA','Uttarakhand','Rajasthan', \n",
    "             'Punjab','Haryana','Himachal Pradesh', \n",
    "             'Madhya Pradesh', 'Jammu & Kashmir', 'Puducherry', \n",
    "             'West Bengal','Bihar', 'Jharkhand', 'Assam', 'Odisha'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making helper functions:\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def maketriple(sent):\n",
    "    triplets = []\n",
    "    for ent in entity_values:\n",
    "        if ent in sent:\n",
    "            sent = sent.replace(ent, \"{\"+ ent+\"}\")\n",
    "    #print(sent)\n",
    "    words = word_tokenize(sent)\n",
    "    #print(words)\n",
    "    keyword_got = False\n",
    "    first_index = -1\n",
    "    \n",
    "    for word in words:\n",
    "        if word == '{':\n",
    "            keyword_got = True\n",
    "            first_index = 0\n",
    "            continue\n",
    "        if word == \"}\":\n",
    "            keyword_got = False\n",
    "            first_index = -1\n",
    "            continue\n",
    "        if keyword_got == True:\n",
    "            if first_index == 0:\n",
    "                triplets.append(((word , pos_tag([word])[0][-1]),\"B-keyword\"))\n",
    "                first_index += 1\n",
    "            else:\n",
    "                triplets.append(((word,  pos_tag([word])[0][-1]),\"I-keyword\"))\n",
    "        else:\n",
    "            triplets.append(((word, pos_tag([word])[0][-1]), \"O\"))\n",
    "\n",
    "    return triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "training  = [maketriple(line) for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('what', 'WP'), 'O'),\n",
       " (('is', 'VBZ'), 'O'),\n",
       " (('the', 'DT'), 'O'),\n",
       " (('principal', 'NN'), 'B-keyword'),\n",
       " (('place', 'NN'), 'I-keyword'),\n",
       " (('of', 'IN'), 'I-keyword'),\n",
       " (('business', 'NN'), 'I-keyword'),\n",
       " (('of', 'IN'), 'O'),\n",
       " (('ABBOTT', 'NN'), 'B-keyword'),\n",
       " (('INDIA', 'NN'), 'I-keyword'),\n",
       " (('LIMITED', 'JJ'), 'I-keyword'),\n",
       " (('in', 'IN'), 'O'),\n",
       " (('Gujarat', 'NNP'), 'B-keyword')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving these triplet in the text files in Entities folder in the same format\n",
    "data_root_path = \"D:\\Projects\\ConversationaFramework\\Data\\Entities\"\n",
    "limit = len(training)\n",
    "file_index = 0\n",
    "sentence_index = 0\n",
    "i = 0\n",
    "while(i < limit):\n",
    "    i += 1\n",
    "    with open(data_root_path + \"/\"+ str(file_index) +\".txt\", 'a') as file:\n",
    "        sentence_index += 1\n",
    "        for triplet in training[i-1]:\n",
    "            (word, tag), ner = triplet\n",
    "            file.write(str(word + '\\t' + tag+ '\\t' +ner))\n",
    "            file.write('\\n')\n",
    "        file.write('\\n')\n",
    "    if sentence_index == 25:\n",
    "        sentence_index = 0\n",
    "        file_index += 1\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For KPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_path = \"D:\\Projects\\ConversationaFramework\\Data\\Domains\\GST\\Intents\\getkpiinfo\"\n",
    "\n",
    "import os\n",
    "# extract only 2 files for now:\n",
    "data = []\n",
    "filenames = os.listdir(root_data_path)\n",
    "for filename in filenames:\n",
    "    with open(root_data_path + \"/\" + filename  ) as file:\n",
    "        content = file.readlines()\n",
    "        content = [line.replace('\\n', '') for line in content]\n",
    "        data.extend(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_values = ['M T P','Mtp','MTP', 'mtp', 'mTP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in data:\n",
    "    entity_values.append(' '.join(line.split()[-2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training  = [maketriple(line) for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('how', 'WRB'), 'O'),\n",
       " (('do', 'VB'), 'O'),\n",
       " (('i', 'NN'), 'O'),\n",
       " (('edit', 'NN'), 'O'),\n",
       " (('my', 'PRP$'), 'O'),\n",
       " (('M', 'NN'), 'B-keyword'),\n",
       " (('T', 'NN'), 'I-keyword'),\n",
       " (('P', 'NN'), 'I-keyword'),\n",
       " (('by', 'IN'), 'O'),\n",
       " (('update', 'NN'), 'B-keyword'),\n",
       " (('Customer', 'NN'), 'I-keyword'),\n",
       " (('s', 'NN'), 'O')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving these triplet in the text files in Entities folder in the same format\n",
    "data_root_path = \"D:\\Projects\\ConversationaFramework\\Data\\Entities\"\n",
    "limit = len(training)\n",
    "file_index = len(os.listdir(data_root_path)) + 1\n",
    "sentence_index = 0\n",
    "i = 0\n",
    "while(i < limit):\n",
    "    i += 1\n",
    "    with open(data_root_path + \"/\"+ str(file_index) +\".txt\", 'a') as file:\n",
    "        sentence_index += 1\n",
    "        for triplet in training[i-1]:\n",
    "            file.write(str(word + '\\t' + tag+ '\\t' +ner))\n",
    "            file.write('\\n')\n",
    "        file.write('\\n')\n",
    "    if sentence_index == 25:\n",
    "        sentence_index = 0\n",
    "        file_index += 1\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
